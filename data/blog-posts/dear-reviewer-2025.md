---
title: "Dear Reviewer"
subtitle: "A Cover Letter"
date: "11-09-2025"
author: "Velitchko Filipov"
hashtags:
  - Peer Review
categories:
  - Research & Rants
excerpt: "A reflection on reviewer fatigue, rushed reviews, and what it means to review with care"
featured: true
---

### Dear Reviewer,

You seem tired. Honestly, I get it. I am <mark>too.</mark>

---

Every year deadlines pile up, papers multiply, and somehow we act like we can keep up.  
Reviewing becomes another item on the todo list — among meetings, proposals, teaching, and whatever else comes up.  

We call it **a service**, put it on our resumes, volunteer for it, act as if it were a noble thing we do.  

The invisible work that keeps our conferences, journals, and quality standards up and running.

Lately, it feels like it's something we do because we **have to** — it’s expected, not because we **want to** or have the time/energy to do it well.

---

It's not that every review is bad.   
I’ve received reviews that were thoughtful and constructive and helped me improve my papers.   
You know, the reviews that make you think about your framing and contribution. 

The reviews that actively help and suggest how you can improve your argumentation.
The ones that make you think 

> *This reviewer really took the time and appreciated my article.*

But I’ve also received reviews that were clearly written in a rush.  
A line or two that completely missed the point.  
The reviews that make me sit there, staring at the screen, wondering:

> *Why did I spend so much time writing this?*  
> *What do I do now?*  
> *Where do I go from here?*

---

Well, I hope I'm not alone in this. 
And, I really don't want to think that any of those reviewers are malicious or incompetent.<sup>[1](#note-1)</sup>


They're probably just **overworked, burnt out, and still trying to do the right thing** in a system that doesn't give them time to.<sup>[3](#note-3)</sup>  

I've experienced it too: reviewing paper after paper, telling myself I'm objective while my focus is elsewhere.

That's what makes it hard to be angry.  
Because it's not just them, **it's me and it's you, it's all of us.**<sup>[9](#note-9)</sup>

We're running on fumes and pretending it's fine.

---

We hold these venues we review for as the gold standard of quality:  

> *"The best papers".*  
> *"The most rigorous reviews".*  
> *"The most prestigous venue".*  

But how rigorous can that process be when half of us are already burned out and the rest are just pretending we’re not?

Good things take time.  
**Good work takes time.**  
Reading carefully takes time.  
Writing a thoughtful and constructive review <mark>takes time</mark>.<sup>[10](#note-10)</sup>  

However, time has become the one thing **nobody has anymore**.  

We’re all in a rush to keep up, to show we’re contributing. 
Even if that means doing things halfway.

---

When I look at the reviews I’ve received, I can tell which ones were written with care and which ones were just box-ticking exercises.  

You know the ones:  

> *"Lacks novelty".*  
> *"Unclear contribution".*  
> *"Needs more evaluation".*  
> *"Related work missing".*  

Oh and don't forget *"scalability & generalizability"*.

Yeah, been there.

It’s not reviewing. It’s something you do to say you did it.  
It helps no one. **<mark>Not a soul.</mark>**

---

And yet, the cycle continues.  
Papers get submitted, reviewed, accepted, published, and celebrated.  
Venues remain prestigious, because, well, they’ve always been prestigious.  

We talk about acceptance rates as if they mean something more than how well you survived *reviewer roulette*.

---

I don't say this to be cynical.  
I believe peer review matters — but if we're being honest, it's not working as intended.<sup>[2](#note-2), [3](#note-3)</sup>  

> *The system isn't failing because people stopped caring.*  
> *It's failing because nobody has the time to care properly anymore.*  
> *The problem isn't apathy, it's <mark>bandwidth</mark>.*

---

I keep thinking we could learn something from how software communities handle this.  

Imagine if we treated papers like pull requests.  

> You open a pull request and someone checks it out and says:  
> "Looks interesting. Maybe clean up this section, clarify that function, fix this bug".  
> No one replies with "Reject".  

They point out what’s broken and how to fix it.  
The goal isn’t to be destructive, it’s to create something together and <mark>get the good to emerge.</mark>

---
Funnily enough, these are the things already described in most reviewer guidelines:<sup>[4](#note-4)</sup>  

> *What would make this contribution stronger?*  
> *How can I help it get there?*

Now imagine if we actually reviewed papers like that.  
That was the idea behind *open review*, but for whatever reason we havent quite yet picked that up.<sup>[6](#note-6), [7](#note-7)</sup>

---

To me, personally, it kind of feels like that meme, [you know the one](https://programmerhumor.io/memes/lgtm).   
someone reviewing a pull request on GitHub, leaving five nitpicky comments about a variable’s name but waving through a hundred lines of regex with "LGTM".  

It’s funny, until you realize that’s exactly what happens in paper reviews too.  

Entire conceptual frameworks get brushed off with *"unclear contribution"*,  
while three sentences debate the use of a term (*is network dynamics plural or not?*).

---

What I wish, really, is that reviewing worked more like a good code review.  

> You don’t reject a pull request; you help it merge cleanly.  
> You point out what’s broken — maybe even how to fix it.  

That’s not lowering the bar.  
That’s **raising the quality of the conversation** and helping someone get better.

---

Maybe part of the problem is that we only ever talk about reviewing as an obligation — *a service to the community*.  

But what if we reframed it?

Not as something we *do for* the community, but as something we *do to build* the community.  

What if reviewing was less about gatekeeping and more about **lifting each other up**?  
About making the work better, the ideas sharper, the conversation richer.

We never talk about it as a *skill* — a skill that can be taught, practiced, and improved.

We expect junior researchers to just pick it up and run with it.    
Meanwhile senior researchers drown in review requests they no longer have time to do properly.  

The truth is, many seniors assign the reviews to juniors anyway.  
Done quietly, invisibly.  

It works, I guess?  

How about if we were more open about this?  
Identify the problem, discuss potential solutions, and try them out to figure out what works.  

---

I know, easier said than done.  
There are practical constraints, and the community **is** trying to fix some of this.  

I've seen discussions about reviewer fatigue, surveys about experiences, and new experiments to measure quality and load.<sup>[5](#note-5), [8](#note-8)</sup>

But here’s the part that still sticks with me:

> *The best reviews I’ve ever received didn’t tell me I was wrong.*  
> *They told me what was wrong — and how to make it better.*  
> *They weren’t written to judge.*  
> *They were written to <mark>help and improve.</mark>*

---

If we want our papers to be worth reading,  
our reviews have to be **worth writing**.

So, dear Reviewer — if you’re tired, **decline the next review request.**<sup>[3](#note-3)</sup>  
It’s fine.  

The system won’t collapse because you didn’t add one more rushed review to the pile.  

And when you do have time, when you do have space —  come back and review with care.  
Review like someone who wants the paper to succeed.

*I’ll do the same.*

---

I don’t have a solution. But I think talking about it matters.  

How and what we review shapes what gets published, what ideas survive, and ultimately what kind of community we become.

> *I wonder if people in other fields feel the same (outside of VIS & HCI).*  
> *Do you?*  
> *Does peer review still feel constructive where you are,*  
> *or is everyone just trying to survive the conference cycle?*


**Sincerely,**  
*Another reviewer.*

---

## Notes & References

Below is a list of references so you can scan everything in one place. 

<ol>
  <li id="note-1">
    <strong>Dear Reviewer 2: Go F' Yourself</strong> — <em>Empirical test of the Reviewer #2 myth.</em><br />
    <a href="https://onlinelibrary.wiley.com/doi/10.1111/ssqu.12824" target="_blank" rel="noopener noreferrer">https://onlinelibrary.wiley.com/doi/10.1111/ssqu.12824</a><br />
    Extended: Challenges the cultural narrative that "Reviewer 2" is uniquely harsh; shows variance is normal and our storytelling amplifies perceived negativity.
  </li>
  <li id="note-2">
    <strong>The Peer Review Crisis</strong> — <em>Peer review framed as an evolutionary pressure point.</em><br />
    <a href="https://onlinelibrary.wiley.com/doi/10.1111/hequ.12511" target="_blank" rel="noopener noreferrer">https://onlinelibrary.wiley.com/doi/10.1111/hequ.12511</a><br />
    Extended: Argues current strain can catalyze reforms: bias reduction, inclusivity measures, and structural recognition of reviewing as core scholarly labor.
  </li>
  <li id="note-3">
    <strong>Reviewer Fatigue</strong> — <em>Workload saturation threatens quality sustainability.</em><br />
    <a href="https://journals.sagepub.com/doi/full/10.1177/03635465231210848" target="_blank" rel="noopener noreferrer">https://journals.sagepub.com/doi/full/10.1177/03635465231210848</a><br />
    Extended: Documents growing mismatch between submission volume and stable reviewer pools; highlights risks of rushed, low-fidelity assessments.
  </li>
  <li id="note-4">
    <strong>IEEE Reviewer Ethics / Guidelines</strong> — <em>Normative template for constructive evaluation.</em><br />
    <a href="https://tc.computer.org/vgtc/conferences/ethics-guidelines/reviewer-ethics" target="_blank" rel="noopener noreferrer">https://tc.computer.org/vgtc/conferences/ethics-guidelines/reviewer-ethics</a><br />
    Extended: Emphasizes objectivity, actionable feedback, time investment, and professional integrity—often cited but unevenly internalized.
  </li>
  <li id="note-5">
    <strong>IEEE VIS 2025 Panel: Reviewing on a Path to Self-Destruction?</strong> — <em>Community self-reflection on sustainability.</em><br />
    <a href="https://ieeevis.org/year/2025/program/session_panel7.html" target="_blank" rel="noopener noreferrer">https://ieeevis.org/year/2025/program/session_panel7.html</a><br />
    Extended: Explores systemic pressure, reviewer load metrics, and potential cultural adjustments (mentorship, expectation calibration).
  </li>
  <li id="note-6">
    <strong>Transparent Peer Review for All (Nature Communications)</strong> — <em>Advocacy for open accountability.</em><br />
    <a href="https://www.nature.com/articles/s41467-022-33056-8" target="_blank" rel="noopener noreferrer">https://www.nature.com/articles/s41467-022-33056-8</a><br />
    Extended: Argues transparency increases trust, mitigates bias opacity, and clarifies revision pathways; notes adoption barriers.
  </li>
  <li id="note-7">
    <strong>OpenReview Platform</strong> — <em>Infrastructure for publicly visible reviews.</em><br />
    <a href="https://openreview.net/" target="_blank" rel="noopener noreferrer">https://openreview.net/</a><br />
    Extended: Enables versioned discourse and community oversight; cultural inertia and uneven incentives slow broader uptake.
  </li>
  <li id="note-8">
    <strong>IEEE VIS Review System Changes (2025)</strong> — <em>Incremental process evolution.</em><br />
    <a href="https://ieeevis.org/year/2025/blog/vis-2025-OPC-blog-Changes" target="_blank" rel="noopener noreferrer">https://ieeevis.org/year/2025/blog/vis-2025-OPC-blog-Changes</a><br />
    Extended: Details experiments in reviewer assignment, mentoring pipelines, and tracking burden to counter fatigue and uneven quality.
  </li>
  <li id="note-9">
    <strong>Academic Life Getting Harder (Glen O'Hara)</strong> — <em>Macro-level stressors and systemic strain.</em><br />
    <a href="https://voicesofacademia.com/2024/04/05/its-not-your-fault-that-academic-life-is-getting-harder-by-glen-ohara/" target="_blank" rel="noopener noreferrer">https://voicesofacademia.com/2024/04/05/its-not-your-fault-that-academic-life-is-getting-harder-by-glen-ohara/</a><br />
    Extended: Attributes difficulty to structural under-resourcing, expansion of administrative overhead, and normative productivity escalation—not personal failure.
  </li>
  <li id="note-10">
    <strong>Slow Science Manifesto</strong> — <em>Temporal defense of deep scholarship.</em><br />
    <a href="https://slowscience.be/the-slow-science-manifesto-2/" target="_blank" rel="noopener noreferrer">https://slowscience.be/the-slow-science-manifesto-2/</a><br />
    Extended: Advocates protected cognitive time for reflection, iteration, and failure; implicitly critiques accelerated review cycles that erode deliberation.
  </li>
</ol>

---

## How to Use This Post

### Share Your Experience

Feel free to:
- **Quote sections** that resonate with you
- **Share on social media** with `#PeerReview` `#AcademicLife`
- **Start discussions** in your research community
- **Write your own response** — I'd love to read different perspectives

### Take Action

Some concrete steps you can take:

1. **Set boundaries** — Decline reviews when you're overloaded
2. **Ask for more time** — Most editors will grant extensions
3. **Mentor junior reviewers** — Share your reviewing approach
4. **Advocate for change** — Support initiatives that value review quality
5. **Write better reviews** — Be the reviewer you wish you had

### Talk About It!

**Discussion** 
Have thoughts on this? Talk to your colleagues and other researchers in your community!
Found something that resonated (or didn't)? I'm curious to hear from reviewers across different fields. Does your community face similar challenges?